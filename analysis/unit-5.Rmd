---
title: "unit-5"
author: "elliothershberg"
date: "2022-04-25"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

Notes from Unit 5: Statistical Inference

## Introduction to Statistical Inference

- Goal is to make a guess about a population from a sample.
- We can compute sample statistics from data
- Use them to infer population parameters
- Quantify uncertainty about these estimates
- Sample statistic: any summary measure calculated from data
- Population parameter: the true value/true effect in the entire population of interest
- Two possible goals:
  1. Estimation (confidence intervals)
  2. Hypothesis testing (p-values)

## Introduction to the Distribution of a Statistic

- Statistics follow distributions
- Distributions are defined by:
  1. Shape (e.g. normal, T, etc.)
  2. Mean
  3. Standard deviation (called standard error for a statistic)
- Distribution of a statistic is a **theoretical concept**
  - If experiment/sampling is repeated over and over, this would form distribution of values
- Two approaches to determining distribution:
  1. computer simulation
  2. mathematical theory

## Distributions of some common statistics

- correlation coefficients range from -1 to +1.
- randomly sampling from a distribution to see the distribution of the mean.
- Distribution of a sample mean in general:
  - T-distributed
  - Mean approaches the true mean
  - the standard error is: $\frac{s}{\sqrt{n}}$
- Standard error decreases with bigger sample size, and increases with greater trait variability.
- Propertires of distribution of correlation coefficient:
  - Shape is normal for large n, t-distributed for smaller n
  - mean = true correlation coefficient.
  - standard error is approximately $\frac{\sqrt{1 - r^2}}{\sqrt{n}}$
- many statistics have similar properties
- the distributions of ratios are right-skewed, but taking the log makes it normally distributed.
- Summary:
  1. many statistics are normal or t-distributed
  2. the mean is typically the true value
  3. the standard error has different formulas for different statistics.

## Confidence Intervals (estimation)

- Goal is to capture the true effect most of the time.
- Example: a 95% confidence interval should the include the true effect about 95% of the time.
- Can use the standard error to compute this.
  - 95% of values fall within two standard deviation, so +/- (2 * SE).
- Confidence intervals give:
  - a plausible range of values for a population parameter.
  - the precision of an estimate. (when sampling variability is high, the confidence interval is wide to reflect the uncertainty.)
  
## Where Does the Margin of Error Come From in Polls?

- Comes from the confidence interval.
- Proportion is related to the binomial. $\frac{X}{N}$ where X is a binomial.
- Standard error of a proportion is: 

$$\sigma_{\hat p} = \sqrt{\frac{p(1-p)}{n}}$$

- for proportion, mean is the true value, and it is normally distributed.
- An interesting result of this: p(1 - p) reaches a maximum when p = 0.5
- Maximum width of 95% confidence interval is $2*\frac{.5}{\sqrt{N}} = \frac{1}{\sqrt{N}}$.

## Hypothesis Testing (p-values)

- Make a baseline assumption (null hypothesis)
- Simulate a distribution for the test statistic
- Compare the value given the null hypothesis to this distribution
- The p-value is the probablity of see observed value given that the null hypothesis is true.
- P-value = P(observation | null hypothesis)
- Steps:
  1. Define hypothesis (null, alternative)
  2. Specify null distribution
  3. Do an experiment
  4. Calculate p-value of observed value
  5. Reject or fail to reject the null
- Confidence intervals give same information. If a value is outside of confidence interval, p-value is significant.
- two-sided hypothesis test: account for values more extreme in either direction.
- P-values mathematically:

$$
Z = \frac{effect \, size - null \, value}{standard \, error}
$$

- P-values depend on: effect size, sample size, variability.

## HIV Vaccine Trial/Bayesian Inference

- HIV Trial, 51 infections in vaccine group vs. 74 infections in placebo group.
- Turns out this difference equates to a p-value of 0.04.
- Can use Bayes rule to flip conditional probabilities.

## Bayesian Statistics

- In frequentist statistics, the parameter of interest is some fixed value. In Bayesian statistics the parameter is a random variable.
- Prior distribution: expresses beliefs before data.
- Posterior distribution: expresses beliefs after seeing data.
- Tools for Bayesian inference:
  - Estimation: credible interval
  - Hypothesis testing: Bayes factors

## Homework

1.

You measure weight changes in 10 adults who have completed a weight loss program. Because of the small sample size, you will need to use a T value rather than a Z value when building the confidence interval for the mean weight loss. What is the correct T value if you want to build a 99% confidence interval?

```{r hw-1}
qt(1 - 0.01 / 2, df = 9)
```

2.

In a study of 100 adults who completed a weight loss program, 40 lost at least 5 kg. Calculate the 95% confidence interval for the true proportion of adults who lose at least 5 kg using this particular weight loss regimen. Recall that proportions are normally distributed with a standard error of $\sqrt{\frac{p(1-p)}{n}}$. (You may use the observed proportion to calculate the standard error.)

```{r hw-2}
p <- 0.4
n <- 100

se <- sqrt((p * (1 - p) ) / n)

# the z-value for a 95% confidence interval
z <- qnorm(1 - 0.05 / 2)

p - z * se
```

3.

A study of 1000 people who were randomly assigned to receive an experimental vaccine (n=500) or a placebo vaccine (n=500) found that 5% of the vaccinated group developed the disease versus 15% of the placebo group. The observed difference between the groups was thus -10%; and the 95% confidence interval was -6% to -14%. TRUE or FALSE: The reduction in infections in the vaccine group is statistically significant at p< .05.

I think this is true because even outside of the 95% confidence interval, it is a reduction from the control.

4.

Smoker odds ratio:

```{r hw-4}
smoker_p <- 25 / (25 + 15)

smoker_odds <- smoker_p / (1 - smoker_p)

control_p <- 75 / (75 + 85)

control_odds <- control_p / (1 - control_p)

odds_ratio <- smoker_odds / control_odds

odds_ratio
```

5.

```{r hw-5}
ln_odds <- log(odds_ratio)

ln_odds
```

6. 

```{r hw-6}
se <- sqrt(
  (1 / 25) +
    (1 / 15) +
    (1 / 75) +
    (1 / 85)
)

se

z <- qnorm(1 - 0.05 / 2)

print("Lower:")
ln_odds - z * se

print("Upper:")
ln_odds + z * se
```

7.

```{r hw-7} 
print("Lower:")
exp(ln_odds - z * se)

print("Upper:")
exp(ln_odds + z * se)
```

12.

```{r hw-12}
p_data_given_heads <- (0.8)^2

p_data_given_tails <- (0.2)^2

p_data_given_fair <- (0.5)^2

p_total <-
  1/3 * p_data_given_heads +
  1/3 * p_data_given_tails +
  1/3 * p_data_given_fair

# so,

ans <- (0.64 * 1/3) / p_total

ans
```

13.

```{r hw-13}
bayes_factor <-
  p_data_given_heads /
  (p_data_given_tails * 0.5 + p_data_given_fair * 0.5)

bayes_factor
```


15.

```{r hw-15}
(1 / 0.05)^2
```


16.

```{r swim-sim}
mean_diff <- 0
for (i in 1:10000) {
  girls <- rnorm(30, 60, 10)
  boys <- rnorm(30, 60, 10)
  mean_diff[i] <- mean(girls) - mean(boys)
}
```

```{r hw-16}
hist(mean_diff)
```

```{r hw-17}
mean(mean_diff)
```

```{r hw-17-1}
sd(mean_diff)
```

```{r hw-18}
sum(mean_diff > 5) / 10000
```

